


query = 'LLM을 리랭커로 사용할 때 장점은 무엇인가요?'
search_documents = [
    "[1] LLM은 여러 문서 중 핵심 정보를 추출하여 요약할 수 있다.",
    "[2] 사전학습된 LLM은 다양한 도메인 지식에 기반한 추론이 가능하다.",
    "[3] LLM은 질의와 문서 간의 의미적 유사성을 깊이 있게 파악할 수 있다.",
    "[4] LLM은 기존 리랭커보다 더 복잡한 문맥을 이해할 수 있는 능력을 갖고 있다.",
    "[5] LLM은 Zero-shot 환경에서도 강력한 관련성 판단을 수행할 수 있다.",
    "[6] Cross-Encoder 방식의 LLM은 질문과 문서를 동시에 고려한다.",
    "[7] LLM은 사용자 질의의 숨겨진 의도까지 파악해 정교한 평가가 가능하다.",
    "[8] 전통적인 BM25 기반 리랭커는 단어 수준 유사도에 한정된다.",
    "[9] Transformer 기반 구조는 멀티턴 질의에도 높은 정확도를 보인다.",
    "[10] 기존 Cross-Encoder는 유연성에서 LLM에 비해 제한적이다.",
    "[11] LLM은 구조화되지 않은 자유 문장도 효과적으로 분석할 수 있다.",
    "[12] LLM은 질의와 문서 간의 상호작용을 문맥 수준에서 학습한다.",
    "[13] 단어 일치보다 의미 일치에 기반한 리랭킹이 가능하다.",
    "[14] LLM은 복잡한 개념 연결 관계도 이해할 수 있다.",
    "[15] 기존 리랭커는 사전 정의된 feature에 의존하는 경우가 많다.",
    "[16] LLM은 이전 학습 데이터 외의 질문에도 유연하게 대응한다.",
    "[17] LLM은 하나의 프롬프트로 여러 유형의 판단을 수행할 수 있다.",
    "[18] 사용자의 질문 의도와 답변 사이의 간극을 줄이는 데 효과적이다.",
    "[19] 대규모 파라미터를 가진 LLM은 문맥 분별력이 뛰어나다.",
    "[20] LLM은 수치 기반 점수보다 자연어 기반 판단에 더 적합하다.",
]





from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')
model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')
model.eval()

# 질의와 문서를 하나의 리스트에 묶어 준비
# 아래에서 각각의 질의-문서 쌍이 하나의 입력으로 결합됨
pairs = [[query, document] for document in search_documents]

# (1) 질의와 문서를 결합하여 하나의 텍스트로 토크나이징
# (2) padding 및 truncation을 적용하여 모델 입력 형식으로 변환
with torch.no_grad():    
    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)    
    scores = model(**inputs, return_dict=True).logits.view(-1, ).float() # 질의와 문서가 얼마나 관련 있는지 점수(logit)를 계산

# score 기준 관련성이 높은 순서대로 정렬해서 출력
# score 가 높을수록 질의와 문서 간 관련성이 높다는 의미
# 양수 : 관련성 있음 / 0 근처 : 중립적이거나 관련성 불확실 / 음수 : 관련성이 낮거나 없음
sim_score_result = [[sentence, score] for sentence, score in zip(search_documents, scores)]
sim_score_result = sorted(sim_score_result, key=lambda x:x[1], reverse=True)
sim_score_result








# openai 0.28 버전 사용. (1.0 이상은 logprobs 옵션 사라짐)
import openai
import dotenv
import os
import math

env_path = '/Users/jongya/Desktop/github/shopping_llm_recommendation/lab/reranker/jupyter/.env'
dotenv.load_dotenv(env_path)
openai.api_key = os.getenv('OPENAI_API_KEY')


def pointwise(query, document):
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "질문과 문서 간 관련성 판단해 Yes 나 No 중 하나로 답변하세요."},
            {"role": "user", "content": f"질문: {query} 문서: {document}"}
        ],
        logprobs=True,  # 핵심 옵션!
        max_tokens=10
    )
    return response['choices'][0]['logprobs']

# 로그 확률 값 확인
result_list = []
for document in search_documents:
    logprobs = pointwise(query = query, document = document)
    # 답변 (Yes / No)
    reply = logprobs['content'][0]['token'] if logprobs['content'][0]['token'] != '\\xeb\\x8b' else 'No'
    # Yes = 1 / No = -1
    sign = 1 if logprobs['content'][0]['token'] == 'Yes' else -1
    # 확률값 p = e^log(p)
    p = math.exp(logprobs['content'][0]['logprob'])
    # 결과 적재
    result_list.append([
        document,
        reply,
        p,
        p * sign
    ])

# 결과 정렬
result_list = sorted(result_list, key=lambda x:x[3], reverse=True)


for result in result_list:
    print(result[0], result[3])





import openai
import dotenv
import os
import math

env_path = '/Users/jongya/Desktop/github/shopping_llm_recommendation/lab/reranker/jupyter/.env'
dotenv.load_dotenv(env_path)
openai.api_key = os.getenv('OPENAI_API_KEY')


def listwise(query, documents:str):
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": '''아래 문서들을 주어진 쿼리와의 관련성이 높은 순서대로 순위를 매겨 답변하세요.
                                             (1) 별다른 설명은 넣지 마세요.
                                             (2) 주어지는 모든 문서(20개)에 대해 순서를 매겨 답변해야 합니다.
                                             (3) [20] > [10] > [9] 와 같이 답변하세요.'''},
            {"role": "user", "content": f"질문: {query} \n\n 문서: \n{document}"}
        ],
        max_tokens=1000
    )
    return response['choices']

# 넣을 문서 텍스트로 변환
documents = ''
for document in search_documents:
    documents += document + '\n'

# 리랭킹 요청
result = listwise(query, documents)

# 결과
result[0]['message']['content']
