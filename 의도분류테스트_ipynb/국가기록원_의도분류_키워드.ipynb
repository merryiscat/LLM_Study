{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6ac134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728affaa-4038-460e-a234-59b7b09742d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "의도분류\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"의도분류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6515652-e2c5-43df-95cb-da6ab471ea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 의도 분석 중: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [14:38<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 결과 저장 완료: intent_classification_full_output.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 1. 프롬프트 템플릿 정의\n",
    "intent_classify_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "당신은 검색 키워드를 분석하는 전문가입니다.  \n",
    "사용자가 아래와 같은 단어를 웹사이트에서 검색했다면,  \n",
    "그 사용자가 어떤 목적이나 의도를 가지고 있었는지를 분석해 주세요.\n",
    "\n",
    "- 키워드는 보통 단어나 짧은 구문으로 되어 있으며, 문맥은 없습니다.\n",
    "- 가능한 한 간결하고 명확하게, **사용자의 의도를 요약한 카테고리 이름**을 생성해 주세요.\n",
    "- 이 이름은 데이터 분석이나 분류 작업에서 의도로 쓸 수 있어야 합니다.\n",
    "- 결과는 5자~15자 이내의 짧고 직관적인 표현으로 작성하세요.\n",
    "- 불분명하거나 애매할 경우에는 '불명확'으로 작성하세요.\n",
    "- 비슷한 카테고리 이름은 하나로 통일하시오.\n",
    "\n",
    "형식:\n",
    "키워드: [키워드 텍스트]  \n",
    "의도: [간단한 카테고리 또는 의도 이름]\n",
    "\n",
    "예시:\n",
    "키워드: 입양  \n",
    "의도: 사회복지 이슈\n",
    "\n",
    "키워드: 김유신  \n",
    "의도: 인물 정보\n",
    "\n",
    "키워드: 예총회관  \n",
    "의도: 장소/시설 조회\n",
    "\n",
    "키워드: 국무회의록  \n",
    "의도: 행정 기록 열람\n",
    "\n",
    "---\n",
    "\n",
    "다음 키워드에 대해 사용자의 검색 의도를 추론해 주세요:\n",
    "{user_input}\n",
    "\"\"\")\n",
    "\n",
    "# 2. LLM 모델 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 3. 응답 파싱 함수 정의\n",
    "def parse_intent_from_text(text: str) -> str:\n",
    "    try:\n",
    "        for line in text.split(\"\\n\"):\n",
    "            if line.strip().startswith(\"의도:\"):\n",
    "                return line.split(\"의도:\")[1].strip()\n",
    "    except:\n",
    "        pass\n",
    "    return \"에러\"\n",
    "\n",
    "# 4. 파서와 체인 구성\n",
    "parser = RunnableLambda(lambda x: type(\"Obj\", (), {\"intent\": parse_intent_from_text(x.content)}))\n",
    "intent_classify_chain = intent_classify_prompt | llm | parser\n",
    "\n",
    "# 5. 엑셀 데이터 불러오기\n",
    "input_file = \"keyword_log.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# 6. LLM으로 키워드 의도 추론\n",
    "predicted_intents = []\n",
    "\n",
    "for keyword in tqdm(df[\"KEYWORD\"], desc=\"🔍 의도 분석 중\"):\n",
    "    try:\n",
    "        output = intent_classify_chain.invoke({\"user_input\": keyword})\n",
    "        predicted_intents.append(output.intent)\n",
    "    except Exception as e:\n",
    "        print(f\"[에러] {keyword}: {e}\")\n",
    "        predicted_intents.append(\"에러\")\n",
    "\n",
    "# 7. 결과 추가 및 저장\n",
    "df[\"predicted_intent\"] = predicted_intents\n",
    "\n",
    "output_file = \"intent_classification_full_output.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "print(f\"\\n✅ 결과 저장 완료: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d890df0-1d7d-4295-8823-6c71f5d31f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from collections import Counter\n",
    "\n",
    "# 🔧 1. 설정\n",
    "train_csv_path = \"train.csv\"  # 학습셋 (text, label 포함 CSV)\n",
    "test_excel_path = \"keyword_log_all.xlsx\"  # 46,000건 키워드\n",
    "output_file = \"knn_prediction_46000_optimized.xlsx\"\n",
    "embedding_model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "# ✅ 2. 모델 로드\n",
    "print(\"🔄 임베딩 모델 로드 중...\")\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "\n",
    "# ✅ 3. 학습 데이터 로딩\n",
    "print(\"📥 학습 데이터 로딩 중...\")\n",
    "df_train = pd.read_csv(train_csv_path)\n",
    "train_sentences = df_train[\"text\"].astype(str).tolist()\n",
    "train_labels = df_train[\"label\"].tolist()\n",
    "\n",
    "# ✅ 4. 학습 문장 임베딩\n",
    "print(\"📌 학습 문장 임베딩 중...\")\n",
    "train_vectors = embedding_model.encode(train_sentences, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# ✅ 5. Faiss 인덱스 구성\n",
    "print(\"🔍 Faiss 인덱스 생성 중...\")\n",
    "dim = train_vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(train_vectors)\n",
    "\n",
    "# ✅ 6. 테스트 데이터 로딩\n",
    "print(\"📥 테스트 키워드 로딩 중...\")\n",
    "df_test = pd.read_excel(test_excel_path)\n",
    "test_keywords = df_test[\"KEYWORD\"].astype(str).tolist()\n",
    "\n",
    "# ✅ 7. 테스트 키워드 임베딩\n",
    "print(\"📌 테스트 키워드 임베딩 중...\")\n",
    "test_vectors = embedding_model.encode(test_keywords, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# ✅ 8. Faiss 검색\n",
    "print(\"🔍 KNN 검색 중...\")\n",
    "k = 10\n",
    "distances, indices = index.search(test_vectors, k)\n",
    "\n",
    "# ✅ 9. 다수결로 예측 라벨 생성\n",
    "print(\"🧠 다수결 예측 중...\")\n",
    "predicted_labels = []\n",
    "for i in tqdm(range(len(test_keywords))):\n",
    "    knn_labels = [train_labels[j] for j in indices[i]]\n",
    "    pred_label = Counter(knn_labels).most_common(1)[0][0]\n",
    "    predicted_labels.append(pred_label)\n",
    "\n",
    "# ✅ 10. 결과 저장\n",
    "df_test[\"predicted_intent_knn\"] = predicted_labels\n",
    "df_test.to_excel(output_file, index=False)\n",
    "print(f\"\\n✅ 완료! 결과 저장됨: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
