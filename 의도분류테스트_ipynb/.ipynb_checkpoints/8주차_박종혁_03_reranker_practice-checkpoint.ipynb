{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0657a57",
   "metadata": {},
   "source": [
    "## Reranking 실습  \n",
    "\n",
    "### 실습 내용  \n",
    "\n",
    "- 탐색한 Reranking 방식을 직접 수행하봄  \n",
    "- Cross-Encoder 방식, LLM as a Reranker 방식 실습  \n",
    "- Cross-Encoder : Huggingface에 있는 BAAI/bge-reranker-v2-m3 모델    \n",
    "- LLM as a Reranker : OpenAI ChatGPT API 사용  \n",
    "\n",
    "### Jupyter Server 구동  \n",
    "\n",
    "```bash\n",
    "uv run jupyter lab\n",
    "```\n",
    "\n",
    "### 리랭킹 질의 및 검색 결과 문서 데이터셋  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6750130",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'LLM을 리랭커로 사용할 때 장점은 무엇인가요?'\n",
    "search_documents = [\n",
    "    \"[1] LLM은 여러 문서 중 핵심 정보를 추출하여 요약할 수 있다.\",\n",
    "    \"[2] 사전학습된 LLM은 다양한 도메인 지식에 기반한 추론이 가능하다.\",\n",
    "    \"[3] LLM은 질의와 문서 간의 의미적 유사성을 깊이 있게 파악할 수 있다.\",\n",
    "    \"[4] LLM은 기존 리랭커보다 더 복잡한 문맥을 이해할 수 있는 능력을 갖고 있다.\",\n",
    "    \"[5] LLM은 Zero-shot 환경에서도 강력한 관련성 판단을 수행할 수 있다.\",\n",
    "    \"[6] Cross-Encoder 방식의 LLM은 질문과 문서를 동시에 고려한다.\",\n",
    "    \"[7] LLM은 사용자 질의의 숨겨진 의도까지 파악해 정교한 평가가 가능하다.\",\n",
    "    \"[8] 전통적인 BM25 기반 리랭커는 단어 수준 유사도에 한정된다.\",\n",
    "    \"[9] Transformer 기반 구조는 멀티턴 질의에도 높은 정확도를 보인다.\",\n",
    "    \"[10] 기존 Cross-Encoder는 유연성에서 LLM에 비해 제한적이다.\",\n",
    "    \"[11] LLM은 구조화되지 않은 자유 문장도 효과적으로 분석할 수 있다.\",\n",
    "    \"[12] LLM은 질의와 문서 간의 상호작용을 문맥 수준에서 학습한다.\",\n",
    "    \"[13] 단어 일치보다 의미 일치에 기반한 리랭킹이 가능하다.\",\n",
    "    \"[14] LLM은 복잡한 개념 연결 관계도 이해할 수 있다.\",\n",
    "    \"[15] 기존 리랭커는 사전 정의된 feature에 의존하는 경우가 많다.\",\n",
    "    \"[16] LLM은 이전 학습 데이터 외의 질문에도 유연하게 대응한다.\",\n",
    "    \"[17] LLM은 하나의 프롬프트로 여러 유형의 판단을 수행할 수 있다.\",\n",
    "    \"[18] 사용자의 질문 의도와 답변 사이의 간극을 줄이는 데 효과적이다.\",\n",
    "    \"[19] 대규모 파라미터를 가진 LLM은 문맥 분별력이 뛰어나다.\",\n",
    "    \"[20] LLM은 수치 기반 점수보다 자연어 기반 판단에 더 적합하다.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2b7df",
   "metadata": {},
   "source": [
    "## Cross-Encoder 방식  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab6418a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[4] LLM은 기존 리랭커보다 더 복잡한 문맥을 이해할 수 있는 능력을 갖고 있다.', tensor(1.5858)],\n",
       " ['[19] 대규모 파라미터를 가진 LLM은 문맥 분별력이 뛰어나다.', tensor(-3.5372)],\n",
       " ['[7] LLM은 사용자 질의의 숨겨진 의도까지 파악해 정교한 평가가 가능하다.', tensor(-4.7256)],\n",
       " ['[15] 기존 리랭커는 사전 정의된 feature에 의존하는 경우가 많다.', tensor(-4.9283)],\n",
       " ['[20] LLM은 수치 기반 점수보다 자연어 기반 판단에 더 적합하다.', tensor(-5.3553)],\n",
       " ['[1] LLM은 여러 문서 중 핵심 정보를 추출하여 요약할 수 있다.', tensor(-5.3571)],\n",
       " ['[10] 기존 Cross-Encoder는 유연성에서 LLM에 비해 제한적이다.', tensor(-5.4176)],\n",
       " ['[17] LLM은 하나의 프롬프트로 여러 유형의 판단을 수행할 수 있다.', tensor(-5.8128)],\n",
       " ['[11] LLM은 구조화되지 않은 자유 문장도 효과적으로 분석할 수 있다.', tensor(-5.8304)],\n",
       " ['[16] LLM은 이전 학습 데이터 외의 질문에도 유연하게 대응한다.', tensor(-5.8559)],\n",
       " ['[5] LLM은 Zero-shot 환경에서도 강력한 관련성 판단을 수행할 수 있다.', tensor(-6.0521)],\n",
       " ['[13] 단어 일치보다 의미 일치에 기반한 리랭킹이 가능하다.', tensor(-6.1061)],\n",
       " ['[3] LLM은 질의와 문서 간의 의미적 유사성을 깊이 있게 파악할 수 있다.', tensor(-6.5490)],\n",
       " ['[2] 사전학습된 LLM은 다양한 도메인 지식에 기반한 추론이 가능하다.', tensor(-6.7263)],\n",
       " ['[8] 전통적인 BM25 기반 리랭커는 단어 수준 유사도에 한정된다.', tensor(-6.8096)],\n",
       " ['[14] LLM은 복잡한 개념 연결 관계도 이해할 수 있다.', tensor(-7.3066)],\n",
       " ['[6] Cross-Encoder 방식의 LLM은 질문과 문서를 동시에 고려한다.', tensor(-7.8556)],\n",
       " ['[12] LLM은 질의와 문서 간의 상호작용을 문맥 수준에서 학습한다.', tensor(-8.0627)],\n",
       " ['[18] 사용자의 질문 의도와 답변 사이의 간극을 줄이는 데 효과적이다.', tensor(-8.6381)],\n",
       " ['[9] Transformer 기반 구조는 멀티턴 질의에도 높은 정확도를 보인다.', tensor(-10.2660)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\n",
    "model.eval()\n",
    "\n",
    "# 질의와 문서를 하나의 리스트에 묶어 준비\n",
    "# 아래에서 각각의 질의-문서 쌍이 하나의 입력으로 결합됨\n",
    "pairs = [[query, document] for document in search_documents]\n",
    "\n",
    "# (1) 질의와 문서를 결합하여 하나의 텍스트로 토크나이징\n",
    "# (2) padding 및 truncation을 적용하여 모델 입력 형식으로 변환\n",
    "with torch.no_grad():    \n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)    \n",
    "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float() # 질의와 문서가 얼마나 관련 있는지 점수(logit)를 계산\n",
    "\n",
    "# score 기준 관련성이 높은 순서대로 정렬해서 출력\n",
    "# score 가 높을수록 질의와 문서 간 관련성이 높다는 의미\n",
    "# 양수 : 관련성 있음 / 0 근처 : 중립적이거나 관련성 불확실 / 음수 : 관련성이 낮거나 없음\n",
    "sim_score_result = [[sentence, score] for sentence, score in zip(search_documents, scores)]\n",
    "sim_score_result = sorted(sim_score_result, key=lambda x:x[1], reverse=True)\n",
    "sim_score_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0fb03f",
   "metadata": {},
   "source": [
    "## LLM as a Reranker 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3003ff",
   "metadata": {},
   "source": [
    "### Pointwise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4cfb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai 0.28 버전 사용. (1.0 이상은 logprobs 옵션 사라짐)\n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "import math\n",
    "\n",
    "env_path = '/Users/jongya/Desktop/github/shopping_llm_recommendation/lab/reranker/jupyter/.env'\n",
    "dotenv.load_dotenv(env_path)\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "def pointwise(query, document):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"질문과 문서 간 관련성 판단해 Yes 나 No 중 하나로 답변하세요.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"질문: {query} 문서: {document}\"}\n",
    "        ],\n",
    "        logprobs=True,  # 핵심 옵션!\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return response['choices'][0]['logprobs']\n",
    "\n",
    "# 로그 확률 값 확인\n",
    "result_list = []\n",
    "for document in search_documents:\n",
    "    logprobs = pointwise(query = query, document = document)\n",
    "    # 답변 (Yes / No)\n",
    "    reply = logprobs['content'][0]['token'] if logprobs['content'][0]['token'] != '\\\\xeb\\\\x8b' else 'No'\n",
    "    # Yes = 1 / No = -1\n",
    "    sign = 1 if logprobs['content'][0]['token'] == 'Yes' else -1\n",
    "    # 확률값 p = e^log(p)\n",
    "    p = math.exp(logprobs['content'][0]['logprob'])\n",
    "    # 결과 적재\n",
    "    result_list.append([\n",
    "        document,\n",
    "        reply,\n",
    "        p,\n",
    "        p * sign\n",
    "    ])\n",
    "\n",
    "# 결과 정렬\n",
    "result_list = sorted(result_list, key=lambda x:x[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4548a434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] LLM은 질의와 문서 간의 의미적 유사성을 깊이 있게 파악할 수 있다. 0.9999998063873693\n",
      "[4] LLM은 기존 리랭커보다 더 복잡한 문맥을 이해할 수 있는 능력을 갖고 있다. 0.999999448776502\n",
      "[13] 단어 일치보다 의미 일치에 기반한 리랭킹이 가능하다. 0.999999448776502\n",
      "[7] LLM은 사용자 질의의 숨겨진 의도까지 파악해 정교한 평가가 가능하다. 0.999999091165777\n",
      "[19] 대규모 파라미터를 가진 LLM은 문맥 분별력이 뛰어나다. 0.9999989719621736\n",
      "[5] LLM은 Zero-shot 환경에서도 강력한 관련성 판단을 수행할 수 있다. 0.9999984951481292\n",
      "[12] LLM은 질의와 문서 간의 상호작용을 문맥 수준에서 학습한다. 0.9999984951481292\n",
      "[6] Cross-Encoder 방식의 LLM은 질문과 문서를 동시에 고려한다. 0.9999858596583402\n",
      "[2] 사전학습된 LLM은 다양한 도메인 지식에 기반한 추론이 가능하다. 0.9999720323248966\n",
      "[17] LLM은 하나의 프롬프트로 여러 유형의 판단을 수행할 수 있다. 0.9997378641193743\n",
      "[10] 기존 Cross-Encoder는 유연성에서 LLM에 비해 제한적이다. 0.9992882983079844\n",
      "[20] LLM은 수치 기반 점수보다 자연어 기반 판단에 더 적합하다. 0.9980715604067301\n",
      "[11] LLM은 구조화되지 않은 자유 문장도 효과적으로 분석할 수 있다. 0.9975256269669018\n",
      "[18] 사용자의 질문 의도와 답변 사이의 간극을 줄이는 데 효과적이다. 0.9959293105107246\n",
      "[14] LLM은 복잡한 개념 연결 관계도 이해할 수 있다. 0.9820125102859462\n",
      "[16] LLM은 이전 학습 데이터 외의 질문에도 유연하게 대응한다. 0.7772967623259942\n",
      "[15] 기존 리랭커는 사전 정의된 feature에 의존하는 경우가 많다. -0.817571080607408\n",
      "[1] LLM은 여러 문서 중 핵심 정보를 추출하여 요약할 수 있다. -0.9241343417785254\n",
      "[9] Transformer 기반 구조는 멀티턴 질의에도 높은 정확도를 보인다. -0.9914205538951522\n",
      "[8] 전통적인 BM25 기반 리랭커는 단어 수준 유사도에 한정된다. -0.9992891321213027\n"
     ]
    }
   ],
   "source": [
    "for result in result_list:\n",
    "    print(result[0], result[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f11b8",
   "metadata": {},
   "source": [
    "### Listwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770481c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[20] > [1] > [2] > [3] > [4] > [5] > [6] > [7] > [8] > [9] > [10] > [11] > [12] > [13] > [14] > [15] > [16] > [17] > [18] > [19]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "import math\n",
    "\n",
    "env_path = '/Users/jongya/Desktop/github/shopping_llm_recommendation/lab/reranker/jupyter/.env'\n",
    "dotenv.load_dotenv(env_path)\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "def listwise(query, documents:str):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": '''아래 문서들을 주어진 쿼리와의 관련성이 높은 순서대로 순위를 매겨 답변하세요.\n",
    "                                             (1) 별다른 설명은 넣지 마세요.\n",
    "                                             (2) 주어지는 모든 문서(20개)에 대해 순서를 매겨 답변해야 합니다.\n",
    "                                             (3) [20] > [10] > [9] 와 같이 답변하세요.'''},\n",
    "            {\"role\": \"user\", \"content\": f\"질문: {query} \\n\\n 문서: \\n{document}\"}\n",
    "        ],\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response['choices']\n",
    "\n",
    "# 넣을 문서 텍스트로 변환\n",
    "documents = ''\n",
    "for document in search_documents:\n",
    "    documents += document + '\\n'\n",
    "\n",
    "# 리랭킹 요청\n",
    "result = listwise(query, documents)\n",
    "\n",
    "# 결과\n",
    "result[0]['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
