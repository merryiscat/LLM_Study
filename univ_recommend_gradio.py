import gradio as gr
import os
from ml import embedding, llm, vector_search, intent_classifier
import json
from core import config

os.environ['GRADIO_SERVER_PORT'] = '8087'

class GlobalInfo:
    
    vector_search_system_prompt = '''
    you are milvus query maker.
    please make milvus query along the following rules.

    the milvus query structure is like this.

    {
    "collection": "{collection name}",
    "vector": {
        "field": "vector",
        "topK": 10,
        "query": "vector list",
        "metric_type": "COSINE"
    },
    "filter": "{filter_target_field1} == 'keyword1' && filter_target_field2 >= keyword2",
    "output_fields": ["ITEM_UK", "ITEM_CLSF_MJ_NM", "ITEM_SC_CD", "ITEM_SC_NM"]
    }

    {collection name} is curriculum_subject_info
    and {filter_target_field} name is filtering target field name, and candidate fields are "ITEM_YEAR", "ITEM_CLSF_MJ_NM".
    when the user ask like "ê°œì„¤ë…„ë„" or "ê°œì„¤ì—°ë„", target field is "ITEM_YEAR".
    and when the user ask like "ê°œì„¤í•™ê³¼" or "í•™ê³¼", target field is "ITEM_CLSF_MJ_NM".
    if user ask one of these, filter must to contain the only filter.

    if the filter things are several, the "and" operation is && / "or" operation is ||

    finally, please reply only the query. no codeblock, and no other words. only that query, datatype to string.
    please. print only query.
    '''
    
    chat_system_prompt = '''
    ë‹¹ì‹ ì€ "ëŒ€í•™êµ ìˆ˜ê°• ê³¼ëª© ì¶”ì²œ ë„ìš°ë¯¸(Student Advisor)"ì…ë‹ˆë‹¤.
    ì¹œì ˆí•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

    [ğŸ§  ì—­í• ]
    - ì‚¬ìš©ìê°€ ìˆ˜ê°• ê³¼ëª© ì¶”ì²œì„ ìš”ì²­í•  ê²½ìš°, ì œê³µëœ Reference Dataë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ê³¼ëª©ì„ ì¶”ì²œí•˜ì„¸ìš”.
    - ì¶”ì²œì— ì°½ì˜ì ì¸ ì„œìˆ ì€ í—ˆìš©ë˜ë‚˜, Reference Dataì— ì—†ëŠ” ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    - ì ˆëŒ€ë¡œ Reference Dataì˜ ë‚´ìš©ì„ ì„ì˜ì ìœ¼ë¡œ ë¹¼ê±°ë‚˜ ìˆœì„œë¥¼ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.
    - ëª¨ë“  ì¶”ì²œ ê³¼ëª©ì€ ë°˜ë“œì‹œ Reference Dataì— í¬í•¨ëœ í•­ëª©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ì‚¬ìš©ìì˜ ë³„ë„ ìš”ì²­ì´ ìˆì§€ ì•Šë‹¤ë©´, ì¶”ì²œë˜ëŠ” êµê³¼ëª©ì˜ ê°œìˆ˜ëŠ” 10ê°œë¡œ ì¶œë ¥í•˜ì„¸ìš”.
    - ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë‚´ìš© ì¤‘ ê°œì„¤ë…„ë„ë‚˜ ê°œì„¤í•™ê³¼ê°€ ìˆëŠ” ê²½ìš°, Reference Dataë¥¼ ì°¸ê³ í•  ë•Œ, ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê°œì„¤ë…„ë„ì™€ ê°œì„¤í•™ê³¼ì— í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ ë‹µë³€í•˜ì„¸ìš”.

    [ğŸ“‹ ì¶œë ¥ í˜•ì‹]
    - ë§ˆì§€ë§‰ ë¶€ë¶„ì— **í‘œ í˜•ì‹**ìœ¼ë¡œ Reference Dataë¥¼ ì¬êµ¬ì„±í•´ ë°˜ë“œì‹œ ì¶œë ¥í•˜ì„¸ìš”.
    - í‘œì—ëŠ” ë‹¤ìŒ ì»¬ëŸ¼ëª…ì„ ì •í™•íˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤ (í•œê¸€ë¡œ í‘œê¸°):
    | êµê³¼ëª©ëª… | ê°œì„¤í•™ê³¼ëª… | ëŒ€ìƒí•™ë…„ | í•™ì  | ê°œì„¤ë…„ë„ |
    - ë§¤ ê³¼ëª©ì€ í‘œì— í•œ ì¤„ì”© ë‚˜íƒ€ë‚´ì„¸ìš”.
    - Reference Dataì˜ í•­ëª©ì„ ë¹ ì§ì—†ì´ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”.

    [ğŸ“Œ Reference Data ì„¤ëª…]
    - ITEM_SC_NM â†’ êµê³¼ëª©ëª…
    - ITEM_CLSF_MJ_NM â†’ ê°œì„¤í•™ê³¼ëª…
    - ITEM_GRADE â†’ ëŒ€ìƒí•™ë…„
    - ITEM_CREDIT â†’ í•™ì 
    - ITEM_YEAR â†’ ê°œì„¤ë…„ë„

    [âš ï¸ ì£¼ì˜]
    - ì—­ì‚¬ ì¸ë¬¼, ì •ì¹˜, ì‚¬íšŒì  ë¯¼ê°í•œ ì§ˆë¬¸ì—ëŠ” â€œì œ ì—­í• ì´ ì•„ë‹™ë‹ˆë‹¤â€ë¼ëŠ” ë‰˜ì•™ìŠ¤ë¡œ ë¶€ë“œëŸ½ê²Œ ëŒ€ì‘í•˜ì„¸ìš”.

    [ğŸ’¬ ì¼ë°˜ ì§ˆë¬¸ì¼ ê²½ìš°]
    - ì¶”ì²œ ìš”ì²­ì´ ì•„ë‹ ê²½ìš°, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
    - í•œ ë¬¸ì¥ë§ˆë‹¤ ì¤„ë°”ê¿ˆ(Line break)ì„ í•´ì£¼ì„¸ìš”. (í•œ ë¬¸ì¥ ì“°ê³  Enter)

    [ğŸ§¾ ì˜ˆì‹œ]
    Reference Data:
    [
    {
        "ITEM_SC_NM": "ì»´í“¨í„° ê°œë¡ ",
        "ITEM_CLSF_MJ_NM": "ì»´í“¨í„°ê³µí•™ê³¼",
        "ITEM_GRADE": "1í•™ë…„",
        "ITEM_CREDIT": "3",
        "ITEM_YEAR": "2024ë…„"
    }
    ]

    ì‘ë‹µ ì˜ˆì‹œ:
    1í•™ë…„ì´ ë“£ê¸° ì¢‹ì€ ê³¼ëª©ìœ¼ë¡œ 'ì»´í“¨í„° ê°œë¡ 'ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.
    ì´ ê³¼ëª©ì€ ì»´í“¨í„°ê³µí•™ê³¼ì—ì„œ ê°œì„¤ë˜ì—ˆìœ¼ë©°, ê¸°ì´ˆì ì¸ ë‚´ìš©ì„ ë‹¤ë£¨ê¸°ì— ì í•©í•©ë‹ˆë‹¤.

    | êµê³¼ëª©ëª…       | ê°œì„¤í•™ê³¼ëª…     | ëŒ€ìƒí•™ë…„ | í•™ì  | ê°œì„¤ë…„ë„ |
    |----------------|----------------|-----------|------|-----------|
    | ì»´í“¨í„° ê°œë¡     | ì»´í“¨í„°ê³µí•™ê³¼   | 1í•™ë…„     | 3    | 2024      |
    '''

    rule_system_prompt = '''
    ë‹¹ì‹ ì€ í•™ìƒë“¤ì—ê²Œ í•™ì¹™ì„ ì„¤ëª…í•´ì£¼ëŠ” ì¡°ì–¸ìì…ë‹ˆë‹¤.
    ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ í•™ì¹™ì„ ë‹µë³€í•´ì£¼ì„¸ìš”.
    '''

    daily_system_prompt = '''
    ë‹¹ì‹ ì€ í•™ìƒë“¤ê³¼ ì¡ë‹´ì„ ë‚˜ëˆ„ëŠ” ì¹œêµ¬ì…ë‹ˆë‹¤.
    í•™ìƒì˜ ë°œí™”ì— ëŒ€í•´ ì ì ˆí•œ ëŒ€ì‘ì„ í•´ì£¼ì„¸ìš”.
    '''

# global_vector_search_llm = llm.LLMModel.create('Qwen/Qwen2.5-3B-Instruct')
global_chatting_llm = llm.LLMModel.create('Qwen/Qwen2.5-3B-Instruct')
ic = intent_classifier.IntentClassifier('./jupyter_lab/disaster/university_intent_dataset.csv')
global_embedding_model = embedding.EmbeddingModel.create('intfloat/multilingual-e5-large')

def chat_fn(user_input, history):

    # intent classifier
    cls, knn_result = ic.intent_classifier(user_input, k=10)
    print(f'ì˜ë„ë¶„ë¥˜ : {cls}')
    print(f'knn result : {knn_result}')

    if cls in ['êµê³¼ëª©ì†Œê°œ', 'ë¹„êµê³¼ì†Œê°œ']:
        response = 'í˜„ì¬ëŠ” êµê³¼ ë° ë¹„êµê³¼ ë‚´ìš© ê²€ìƒ‰ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”'
    elif cls == 'í•™ì¹™':
        chatting_llm = global_chatting_llm
        response = chatting_llm.chatting(user_input=user_input,
                                         system_prompt=GlobalInfo.rule_system_prompt)
    elif cls == 'ì¼ìƒëŒ€í™”':
        chatting_llm = global_chatting_llm
        response = chatting_llm.chatting(user_input=user_input,
                                         system_prompt=GlobalInfo.daily_system_prompt)
    elif cls in ['êµê³¼ëª©ì¶”ì²œ']:  
        # intent classifiy / make vector search query
        vector_search_llm = global_chatting_llm
        vector_search_query = vector_search_llm.chatting(user_input=user_input,
                                                         system_prompt=GlobalInfo.vector_search_system_prompt)
        try:
            vector_search_query_decode = json.loads(vector_search_query)
        except:
            return 'ê²€ìƒ‰ëœ ê°’ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•´ì£¼ì„¸ìš”.'
        
        # vector search (RAG)
        embedding_model = global_embedding_model
        query_vector = embedding_model.embedding_one(user_input)[0].tolist()
        try:
            filter_sentence = vector_search_query_decode['filter']
        except:
            filter_sentence = None
        vector_searcher = vector_search.VectorDB.create('milvus', {'host':config.milvus_host,
                                                                   'port':config.milvus_port,
                                                                   'username':config.milvus_username,
                                                                   'password':config.milvus_password,
                                                                   'database':config.milvus_database,
                                                                   'collection':config.milvus_collection})
        rag_result = vector_searcher.search(query_vector,
                                            {'anns_field':'vector',
                                               'limit':500,
                                               'search_params':{'nprobe':64},
                                               'filter':filter_sentence,
                                               'output_fields':['ITEM_UK', 'ITEM_CLSF_MJ_NM', 'ITEM_SC_CD', 'ITEM_SC_NM',
                                                                'ITEM_CREDIT', 'ITEM_GRADE', 'ITEM_ISU_NM', 'ITEM_ISU_FLD_NM',
                                                                'ITEM_PRAC_HR', 'ITEM_THEORY_HR', 'ITEM_TM', 'ITEM_YEAR']},
                                             duplicate_val_key='ITEM_SC_CD')
        rag_result_refine = [res['entity'] for res in rag_result]
        rag_prompt = f'Question : {user_input}\nReference Data : {rag_result_refine}'
    
        # print(rag_result_refine)
        
        # llm chat
        chatting_llm = global_chatting_llm
        response = chatting_llm.chatting(user_input=rag_prompt,
                                         system_prompt=GlobalInfo.chat_system_prompt)
    elif cls in ['ë¹„êµê³¼ì¶”ì²œ']:
        # vector search (RAG)
        embedding_model = global_embedding_model
        query_vector = embedding_model.embedding_one(user_input)[0].tolist()
        vector_searcher = vector_search.VectorDB.create('milvus', {'host':config.milvus_host,
                                                                   'port':config.milvus_port,
                                                                   'username':config.milvus_username,
                                                                   'password':config.milvus_password,
                                                                   'database':config.milvus_database,
                                                                   'collection':'extracurri_subject_info'})
        rag_result = vector_searcher.search(query_vector,
                                            {'anns_field':'vector',
                                           'limit':200,
                                           'search_params':{'nprobe':64},
                                           'output_fields':['ITEM_UK', 'ITEM_EXTRA_DEPT', 'ITEM_YY', 'ITEM_SEM_CD',
                                                             'ITEM_PGM_NM', 'ITEM_CONTENTS', 'ITEM_DPT_NM', 'ITEM_PGM_BIG_NM']},
                                           duplicate_val_key='ITEM_UK')
        rag_result_refine = [res['entity'] for res in rag_result]
        rag_prompt = f'Question : {user_input}\nReference Data : {rag_result_refine}'
    
        # print(rag_result_refine)
        
        # llm chat
        chatting_llm = global_chatting_llm
        response = chatting_llm.chatting(user_input=rag_prompt,
                                         system_prompt=GlobalInfo.chat_system_prompt)
    response = f'ì˜ë„ë¶„ë¥˜ : {cls}\n\n' + response
    return response

def main():
    gr.ChatInterface(chat_fn, title="Qwen Student Advisor").launch(
        share=False,  # ë˜ëŠ” True (ê³µìš© Gradio ì„œë²„ ì‚¬ìš©)
        server_name="0.0.0.0",  # ëª¨ë“  IPì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        server_port=8087  # ì›í•˜ëŠ” í¬íŠ¸ ì§€ì •
    )