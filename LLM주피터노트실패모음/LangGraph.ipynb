{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817fcff4-ca1d-4fee-b493-732f5e27a685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4442273c-339f-4c8c-bcb5-57baa41435a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "주식분석\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"주식분석\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab1e8ea-a80c-4d38-9067-7f716034ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhy\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1375: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from edges import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0200fcd4-eb13-44d9-b832-ce5309967a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://the-edit.co.kr/65111\",\n",
    "    \"https://blog.naver.com/sud_inc/223539001961?trackingCode=rss\",\n",
    "    \"https://mochaclass.com/blog/직장인을-위한-취미생활-가이드-요즘-취미-트렌드부터-취미-추천까지-7797\",\n",
    "    \"https://www.hankyung.com/article/2024072845441\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2047257d-9c9b-470c-9c44-8f14501c0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1753c05d-38a6-4c8a-ba40-ac888631893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## : 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91cc6c0f-b993-4f8b-8d54-8235a66e916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 기능을 사용하는 Node 설계\n",
    "# 먼저 저장한 DB의 데이터를 불러온다.\n",
    "# 임베딩 함수로는 openai의 임베딩을 사용하였다.\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 해당 툴을 정의하는 것.\n",
    "# 이를 이용하여 LLM에 해당 툴을 결합시킬 수 있음.\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    # 해당 retriever이 tool call에 의해 호출되는 경우, 해당 tool의 이름\n",
    "    \"retrieve_trends\",\n",
    "    # 해당 tool을 호출해야하는 상황을 Agent가 판단할 수 있도록 지시\n",
    "    \"Search for the latest trends in fashion and hobbies and return relevant information.\",\n",
    ")\n",
    "\n",
    "# 해당 툴 노드를 정의하는 것.\n",
    "# Graph에서 사용하기 위해 Node로 만들 필요가 있음.\n",
    "retrieve = ToolNode([retriever_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce0cd75-24b9-424c-80a6-c02a88be7941",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### STATE #####\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class PersonaState(TypedDict):\n",
    "    user_input: str\n",
    "    messages: Annotated[list, add_messages]\n",
    "    character_persona_dict: dict\n",
    "    retrieve_check: bool\n",
    "    retrieval_msg: str\n",
    "    rewrite_query: str\n",
    "    tools_call_switch: Annotated[bool, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c28b57-f809-4a36-9ebd-844ea4c4751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리를 한 번에 모두 로드\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from states import *\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15f2580-80a4-4442-abca-cdb7a2b2df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB 로드\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25bc371b-3d0b-43ca-8310-61a563710ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool의 경우 llm에 bind 용도로 사용할 것이고\n",
    "tool = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62cdc9f-142d-4528-8a97-c3e405dadfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_search_tool의 경우 직접 invoke를 통해 검색 결과를 받아올 것이다.\n",
    "# 해당 함수의 경우에는 재검색이 요청된 경우에 사용하도록 한다.\n",
    "web_search_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bbbc687-59a7-4f61-ae02-04df79f6f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드 1-1. 검색용 노드\n",
    "tool_node = ToolNode(tools=[tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4621b9b-ae88-4278-a519-f82ccf1a1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색용 RAG 툴 로드하고 노드만듦\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_trends\",\n",
    "    \"Search for the latest trends in fashion and hobbies and return relevant information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1262f935-02f7-4a99-9764-08f063a22e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드 1-2. RAG용 노드.\n",
    "retrieve = ToolNode([retriever_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56b0b14-b20a-4206-ac7a-85e2dfddb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개 툴 엮어서 리스트 만듦.\n",
    "tools = [tool, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab5bd04-073b-4f36-8a49-712e7a9e9d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 툴 2개가 bind된 character_make_node\n",
    "character_model = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "character_model_with_tools = character_model.bind_tools(tools)\n",
    "\n",
    "def character_make_node(state: PersonaState):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"\"\"\n",
    "        You are an expert in creating characters for fiction.\\n\n",
    "        Whatever input the user is presented with, you must return a description of the completed character.\\n\n",
    "        If no information is available, randomly generate and return the character's attributes.\\n\n",
    "        Based on the values entered by the user, envision and present the character, including the character's age, gender, job, location, interests, hobbies and etc.\\n\n",
    "        The returned value must be in Korean.\\n\n",
    "        \"\"\"),\n",
    "        (\"human\", \"Input: {human_input}\\n Retrieve: {context}\"),\n",
    "    ])\n",
    "    prompt_with_tools = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"\"\"\n",
    "        You are an expert in creating characters for fiction.\\n\n",
    "        Whatever input the user is presented with, you must return a description of the completed character.\\n\n",
    "        If no information is available, randomly generate and return the character's attributes.\\n\n",
    "        Based on the values entered by the user, envision and present the character, including the character's age, gender, job, location, interests, hobbies and etc.\\n\n",
    "        If you have difficulty creating an appropriate character, use an online search to solve the problem.\\n\n",
    "        The returned value must be in Korean.\\n\n",
    "        \"\"\"),\n",
    "        (\"human\", \"Input: {human_input}\\n Retrieve: {context}\"),\n",
    "    ])\n",
    "    messages_list = state['messages']\n",
    "    last_human_message = next((msg for msg in reversed(messages_list) if isinstance(msg, HumanMessage)), None).content\n",
    "    last_msg = state['messages'][-1].content\n",
    "    \n",
    "    if last_human_message == last_msg:\n",
    "        last_msg = \"\"\n",
    "        print(f\"==================================== INPUT ====================================\\nHuman Input: {last_human_message}\")\n",
    "    else:\n",
    "        try:\n",
    "            last_msg_data = json.loads(state['messages'][-1].content)\n",
    "            last_msg = \"\\n\\n\".join([d[\"content\"] for d in last_msg_data])\n",
    "        except:\n",
    "            ...\n",
    "        print(f\"==================================== INPUT ====================================\\nHuman Input: {last_human_message}\\nContext: {last_msg}\")\n",
    "    \n",
    "    if state['tools_call_switch']:\n",
    "        chain_with_tools = prompt_with_tools | character_model_with_tools\n",
    "        response = chain_with_tools.invoke({\"human_input\": last_human_message, \"context\": last_msg})\n",
    "        \n",
    "        if hasattr(response, \"tool_calls\") and len(response.tool_calls) > 0 and (response.tool_calls[0][\"name\"]) == \"tavily_search_results_json\":\n",
    "            print(\"================================ Search Online ================================\")\n",
    "            tool_switch = False\n",
    "        elif hasattr(response, \"tool_calls\") and len(response.tool_calls) > 0 and (response.tool_calls[0][\"name\"]) == \"retrieve_trends\":\n",
    "            print(\"=============================== Search Retrieval ===============================\")\n",
    "            tool_switch = False\n",
    "        else:\n",
    "            print(\"============================= Chracter Information =============================\")\n",
    "            tool_switch = False\n",
    "            print(response.content)\n",
    "            \n",
    "    else:\n",
    "        chain = prompt | character_model\n",
    "        response = chain.invoke({\"human_input\": last_human_message, \"context\": last_msg})\n",
    "        print(\"============================= Chracter Information =============================\")\n",
    "        tool_switch = False\n",
    "        print(response.content)\n",
    "\n",
    "    return {\"messages\": [response], \"user_input\": last_human_message, \"tools_call_switch\": tool_switch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e5b6e66-55cc-4c6b-9bee-b655592cc52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhy\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1375: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 노드 1-3. RAG 검증노드\n",
    "# 노드 1-2의 Tools Output을 받아서, User Input에 잘 맞는지 검증해서 Yes Or No로 대답함.\n",
    "# 만약 Yes라면 그대로 다시 Character Make Node로 보내서 최종 답변을 생성하도록 하고\n",
    "# 아니라면 검색을 진행하고 새로운 값을 받아서 보낼거임.\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score:str = Field(..., description=\"Documents are relevant to the question, 'yes' or 'no'\", enum=['yes', 'no'])\n",
    " \n",
    "rag_check_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "rag_check_model = rag_check_model.with_structured_output(GradeDocuments)\n",
    "\n",
    "def retrieve_check_node(state: PersonaState):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"\"\"\n",
    "            You are a consultation expert who provides appropriate information in response to user input.\n",
    "            Return 'yes' or 'no' if you can provide an accurate answer to the user's question from the given documentation.\n",
    "            If you can't provide a clear answer, be sure to return NO.\n",
    "            \"\"\"),\n",
    "            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User's input: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    retrieval_msg = state['messages'][-1].content\n",
    "    human_msg = state['user_input']\n",
    "    retrieval_grader = prompt | rag_check_model\n",
    "    response = retrieval_grader.invoke({\"document\": retrieval_msg, \"question\": human_msg})\n",
    "    retrieve_handle = response.binary_score\n",
    "    retrieve_check = False\n",
    "    \n",
    "    if retrieve_handle == \"no\":\n",
    "        print(\"=============================== Need to Check ===============================\")\n",
    "        retrieve_check = True\n",
    "    if retrieve_handle == \"yes\":\n",
    "        print(\"============================== No Need to Check =============================\")\n",
    "        \n",
    "    return {\"retrieve_check\": retrieve_check, \"retrieval_msg\": retrieval_msg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca96211d-c9a3-4d4e-8011-65dabeb1deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "# 노드 1-4. 쿼리 재-작성 노드\n",
    "# 노드 1-2에서 산출된 retrieve가 입력값과 적절하게 매치되지 않는 경우, 입력값을 수정하게 됨.\n",
    "# state User_input 이용\n",
    "# 이는 노드 1-3에서 yes를 반환하는 경우에 실행됨.\n",
    "\n",
    "class Rewrite_Output(TypedDict):\n",
    "    \"\"\"\n",
    "    Sturctured_output을 생성하기위한 클래스\n",
    "    \"\"\"\n",
    "    query: Annotated[str, ..., \"Rewritten query to find appropriate material on the web\"]\n",
    "\n",
    "rewrite_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "rewrite_model = rewrite_model.with_structured_output(Rewrite_Output)\n",
    "\n",
    "def rewrite_node(state: PersonaState):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"\"\"\n",
    "            You're an expert in improving search relevance.\\n\n",
    "            Look at previously entered search queries and rewrite them to better find that information on the internet.\n",
    "            \"\"\"),\n",
    "            (\"human\", \"Previously entered search queries: \\n{user_input}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    user_input = state['user_input']\n",
    "    rewrite_chain = prompt | rewrite_model\n",
    "    response = rewrite_chain.invoke({\"user_input\": user_input})\n",
    "    rewrited_query = response['query']\n",
    "    print(f\"================================ Rewrited Query ================================\\nRewritted Query: {rewrited_query}\")\n",
    "\n",
    "    return {\"rewrite_query\": rewrited_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b5d6ab0-4e0c-4a60-a87d-51dbe383a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "# 노드 1-5. 재작성된 쿼리를 이용해서 인터넷 검색하는 노드\n",
    "\n",
    "def rewrite_search_node(state: PersonaState):\n",
    "    print(\"================================ Search Web ================================\")\n",
    "    docs = web_search_tool.invoke({\"query\": state['rewrite_query']})\n",
    "    web_results = \"\\n\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = web_results + \"\\n\\n\" + state['retrieval_msg']\n",
    "    # print(web_results)\n",
    "\n",
    "    new_messages = [ToolMessage(content=web_results, tool_call_id=\"tavily_search_results_json\")]\n",
    "            \n",
    "    return {\"messages\": new_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06b9b97e-e6cd-4a63-b709-301aa1f42e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라우팅 함수를 수정해주자.\n",
    "# 검색이 필요한 것인지, 아니면 RAG가 필요한 것인지 탐색!\n",
    "def simple_route(state: PersonaState):\n",
    "    \"\"\"\n",
    "    Simplery Route Tools or Next or retrieve\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0 and ai_message.tool_calls[0][\"name\"] == \"tavily_search_results_json\":\n",
    "        # print(\"Tavily Search Tool Call\")\n",
    "        return \"tools\"\n",
    "    elif hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0 and ai_message.tool_calls[0][\"name\"] == \"retrieve_trends\":\n",
    "        # print(\"Retrieve Call\")\n",
    "        return \"retrieve\"\n",
    "\n",
    "    return \"next\"\n",
    "\n",
    "# 여기서는 RAG가 괜찮은지 검증하여 반환.\n",
    "def retrieve_route(state: PersonaState):\n",
    "    \"\"\"\n",
    "    RAG Need Check?\n",
    "    \"\"\"\n",
    "    if state['retrieve_check']:\n",
    "        return \"rewrite\"\n",
    "\n",
    "    return \"return\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "008b0ca3-fe15-4715-aff8-2e4ee9ed70d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Node `User Input` already present.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 마지막으로 지금까지 만든 노드를 모두 넣어준다.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m graph_builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser Input\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input_node)\n\u001b[0;32m      3\u001b[0m graph_builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharacter Make\u001b[39m\u001b[38;5;124m\"\u001b[39m, character_make_node)\n\u001b[0;32m      4\u001b[0m graph_builder\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharacter Retrieve Check\u001b[39m\u001b[38;5;124m\"\u001b[39m, retrieve_check_node)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\graph\\state.py:352\u001b[0m, in \u001b[0;36mStateGraph.add_node\u001b[1;34m(self, node, action, metadata, input, retry)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` already present.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;241m==\u001b[39m END \u001b[38;5;129;01mor\u001b[39;00m node \u001b[38;5;241m==\u001b[39m START:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is reserved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Node `User Input` already present."
     ]
    }
   ],
   "source": [
    "# 마지막으로 지금까지 만든 노드를 모두 넣어준다.\n",
    "graph_builder.add_node(\"User Input\", user_input_node)\n",
    "graph_builder.add_node(\"Character Make\", character_make_node)\n",
    "graph_builder.add_node(\"Character Retrieve Check\", retrieve_check_node)\n",
    "graph_builder.add_node(\"Rewrite Tool\", rewrite_node)\n",
    "graph_builder.add_node(\"Rewrite-Search\", rewrite_search_node)\n",
    "graph_builder.add_node(\"Tavily Search Tool\", tool_node)\n",
    "graph_builder.add_node(\"RAG Tool\", retrieve)\n",
    "\n",
    "graph_builder.add_edge(START, \"User Input\")\n",
    "graph_builder.add_edge(\"User Input\", \"Character Make\")\n",
    "graph_builder.add_edge(\"Tavily Search Tool\", \"Character Make\")\n",
    "graph_builder.add_edge(\"RAG Tool\", \"Character Retrieve Check\")\n",
    "graph_builder.add_edge(\"Rewrite Tool\", \"Rewrite-Search\")\n",
    "graph_builder.add_edge(\"Rewrite-Search\", \"Character Make\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"Character Make\",\n",
    "    simple_route,\n",
    "    {\"tools\": \"Tavily Search Tool\", \"next\": \"Persona Setup\", \"retrieve\": \"RAG Tool\"}\n",
    ")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"Character Retrieve Check\", \n",
    "    retrieve_route, \n",
    "    {\"rewrite\": \"Rewrite Tool\", \"return\": \"Character Make\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc3b9b-bee4-4726-9635-31b73ae1cea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
